---
title: "This one is screwed"
author: "Joe Brown"
date: "2025-01-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Note

Used analysis numbering begins at 02,; 01 was an initial exploratory analysis not used in the final project.

## Goal

1. Identify the 2100 temperature anomaly of 15000 member ensemble at quantiles. 

2. Identify the quantiles at which the Hector 15000 member ensemble has temperatures that match AR6 percentiles.

## Load data

Load full result:
```{r}
# load result 
model_result <- readRDS("output/model_results/initial_result5k.RDS")

```

source helper functions:
```{r}
# source utils
source("set-up/utils.R")
source("set-up/libraries.R")
```

Subset data to include `global_tas` and normalize data to AR6 reference period:
```{r}
## subset to only include global_tas
gsat_data <- lapply(names(model_result), function(scenario_name) {
  
  # copy data
  df <- model_result[[scenario_name]]
  
  # Count NAs before removal
  na_count <- sum(is.na(df))
  print(paste("Number of NAs in", scenario_name, "data:", na_count))
  
  # subset data to include only global temperature
  subset_data = subset(na.omit(df), variable == "global_tas")
  
  subset_data$scenario <- scenario_name
  
  return(subset_data)
})

## copy list element names
names(gsat_data) <- names(model_result)

## normalize gsat data
gsat_data_normalized <- lapply(gsat_data, function(df) {
  
  normalize_to_reference(df, reference_start = 1995, reference_end = 2014)
  
}) 

```

# Compute warming metrics

Compute warming metrics for AR6 time periods:

```{r}

# initialize metric 
long_warming_metric <- new_metric(var = 'global_tas', years = 2081:2100, op = median)
mid_warming_metric <- new_metric(var = 'global_tas', years = 2041:2060, op = median)
short_warming_metric <- new_metric(var = 'global_tas', years = 2021:2040, op = median)

metric_list <- list('2081-2100' = long_warming_metric, 
                    '2041-2060' = mid_warming_metric, 
                    '2021-2040' = short_warming_metric)

# compute mid-term warming metrics
warming_result <- lapply(names(gsat_data_normalized), function(scenario_name) {
  
  # copy gsat data
  df <- gsat_data_normalized[[scenario_name]]
  
  # compute metrics for each term length for current scenario element 
  metrics_by_term <- lapply(names(metric_list), function(term_length) {

    # copy metric data
    metric <- metric_list[[term_length]]
        
    # compute metrics
    result <- metric_calc(df, metric)
    
    # add term_length column
    result$term_length <- term_length
  
    return(result)  
  })  
  
  # rbind term_lengths for currrent scenario element
  metrics_by_scenario <- do.call(rbind, metrics_by_term)
  
  # Add column for scenario name
  metrics_by_scenario$scenario <- scenario_name
  
  return(metrics_by_scenario)

})

# bind metric result for all scenarios to one df
metrics_df <- do.call(rbind, warming_result)

```

# Compute temp anomaly quantiles

Temperature anomaly quantiles from Hector 15000 member ensemble:

```{r}

warming_quantiles_hector <- metrics_df %>% 
  group_by(scenario, term_length) %>% 
  summarize(
    lower = quantile(metric_result, probs = 0.15), 
    central = quantile(metric_result, probs = 0.5), 
    upper = quantile(metric_result, probs = 0.95),
  .groups = "drop")

```

```{r}
# IPCC data 
source("data/ipcc_data/ipcc_warming_data.R")
IPCC_data <- subset(IPCC_data, name == "WG1 Assessed Range")

# Load the IPCC dataset (modify if needed)
target_IPCC_quantiles_df <- IPCC_data %>%
  rename(year_range = metric_criterion, low = lower_ci, high = upper_ci) %>%
  select(scenario, year_range, low, median, high) # Keep only necessary columns

```

For one scenario-term combination find in what percentile the IPCC value exists in the Hector distribution:
```{r}
# metric split - split by scenario and term length
metric_split <- split(metrics_df, list(metrics_df$scenario, metrics_df$term_length))

# Example distribution (your Hector ensemble result)
hector_distribution <- metric_split$`SSP1-1.9.2021-2040`$metric_result

# Value to find the quantile for.
value_to_find <- 0.38

# Sort the distribution to prepare for the ECDF
sorted_distribution <- sort(hector_distribution)

# Generate the empirical cumulative distribution function (ECDF)
ecdf_function <- ecdf(sorted_distribution)

# Use the ECDF to find the cumulative probability (percentile) for the given value
percentile_of_value <- ecdf_function(value_to_find)

# Print the percentile (cumulative probability) for the value
cat("The value", value_to_find, "is at approximately", round(percentile_of_value * 100, 2), "percentile of the distribution.\n")

```
testing across all scenario-term combinations:
```{r}
# split IPCC by scenario and term length
ipcc_data_test <- split(IPCC_data, list(IPCC_data$scenario, IPCC_data$metric_criterion))
# 
# # Iterate through each item in the ipcc_data_test list
# for (scenario_term in names(ipcc_data_test)) {
#   # Extract the corresponding Hector distribution (metric_result)
#   hector_distribution <- metric_test[[scenario_term]]$metric_result
#   
#   # Sort the Hector distribution
#   sorted_distribution <- sort(hector_distribution)
#   
#   # Create the ECDF function
#   ecdf_function <- ecdf(sorted_distribution)
#   
#   # Extract the corresponding IPCC values (lower, median, upper)
#   ipcc_values <- ipcc_data_test[[scenario_term]]
#   lower_ci <- ipcc_values$lower_ci
#   median_ci <- ipcc_values$median
#   upper_ci <- ipcc_values$upper_ci
#   
#   # Compute the percentiles for the lower, median, and upper IPCC values
#   lower_percentile <- ecdf_function(lower_ci)
#   median_percentile <- ecdf_function(median_ci)
#   upper_percentile <- ecdf_function(upper_ci)
#   
#   # Print the percentiles for this scenario and term length
#   cat("Scenario:", scenario_term, "\n")
#   cat("Lower CI (", lower_ci, ") is at approximately", round(lower_percentile * 100, 2), "percentile\n")
#   cat("Median CI (", median_ci, ") is at approximately", round(median_percentile * 100, 2), "percentile\n")
#   cat("Upper CI (", upper_ci, ") is at approximately", round(upper_percentile * 100, 2), "percentile\n")
#   cat("\n") # To separate results for clarity
# }
```

Using lapply for this:
```{r}
# Use lapply with an anonymous function to calculate percentiles
percentiles_list <- lapply(names(ipcc_data_test), function(scenario_term) {
  
  # Extract the corresponding Hector distribution (metric_result)
  hector_distribution <- metric_split[[scenario_term]]$metric_result
  
  # Sort the Hector distribution
  sorted_distribution <- sort(hector_distribution)
  
  # Create the ECDF function
  ecdf_function <- ecdf(sorted_distribution)
  
  # Extract the corresponding IPCC values (lower, median, upper)
  ipcc_values <- ipcc_data_test[[scenario_term]]
  lower_ci <- ipcc_values$lower_ci
  median_ci <- ipcc_values$median
  upper_ci <- ipcc_values$upper_ci
  
  # Compute the percentiles for the lower, median, and upper IPCC values
  lower_percentile <- ecdf_function(lower_ci)
  median_percentile <- ecdf_function(median_ci)
  upper_percentile <- ecdf_function(upper_ci)
  
  # Return a list with the results for this scenario
  return(list(
    scenario = scenario_term,
    lower_percentile = round(lower_percentile * 100, 2),
    median_percentile = round(median_percentile * 100, 2),
    upper_percentile = round(upper_percentile * 100, 2)
  ))
})

# Print the results
for (result in percentiles_list) {
  cat("Scenario:", result$scenario, "\n")
  cat("Lower CI is at approximately", result$lower_percentile, "percentile\n")
  cat("Median CI is at approximately", result$median_percentile, "percentile\n")
  cat("Upper CI is at approximately", result$upper_percentile, "percentile\n")
  cat("\n") # To separate results for clarity
}
```

## Score Ensembles

Use scoring function in utils to score each ensemble against historical data:
```{r}
# source criterion and scoring 
source("set-up/criterion_scoring_set_up.R")

# Normalize result to 1850-1900 
result_normalized <- lapply(model_result, function(df) {
  
  # normalize result
  result_normalized <- normalize_to_reference(df, reference_start = 1850, reference_end = 1900)
  
  return(result_normalized)
})
  
# calculate scores
scores <- lapply(names(result_normalized), function(scenario_name) {
  
  df <- result_normalized[[scenario_name]]
  
  # score results
  scores <- score_model(df)
  
  scores$scenario <- scenario_name
  
  return(scores)
  
})

score_df <- do.call(rbind, scores)
```

```{r}
# Sort the entire dataset by mc_weight in descending order and get the 5000th largest weight
threshold_weight <- score_df %>%
  arrange(desc(mc_weight)) %>%
  slice(5000) %>%
  pull(mc_weight)  # Extract the mc_weight value at the 5000th position

# Create the plot
ecdf_plot <- ggplot(score_df, aes(x = mc_weight)) +
  geom_point(aes(color = scenario), stat = "ecdf", size = 1) +  # ECDF plot
  geom_vline(xintercept = threshold_weight, linetype = "dashed", color = "red", size = 1) +  
  scale_x_continuous(limits = c(0, 1e-4)) + 
  labs(x = "MC Weight", y = "ECDF", title = "ECDF of Ensemble Weights with Top 5000 Threshold") +
  facet_wrap(~scenario) +
  theme_light() +
  theme(legend.position = "none")
ggsave("figures/ecdf_plot.png", ecdf_plot, width = 10, units = "in")

```


## Subsampling strategy 

```{r}
# Reshape data
metrics_df_wide <- metrics_df %>%
  pivot_wider(
    names_from = term_length, 
    values_from = metric_result
  )

sample_fxn <- function(df, n = 1000, num_reps = 100) {
  
  # Split the data frame by 'scenario'
  split_df <- split(df, df$scenario)
  
  # Use lapply to repeat the sampling 100 times for each scenario
  repeated_samples <- lapply(split_df, function(scenario_df) {
    
    # Use replicate to repeat the sampling process 'num_reps' times
    samples_list <- replicate(num_reps, {
      # Sample row indices (instead of sampling the entire data frame)
      sampled_indices <- sample(nrow(scenario_df), size = n, replace = F)
      
      # Subset the data frame by the sampled indices
      sampled_data <- scenario_df[sampled_indices, ]
      
      return(sampled_data)
    }, simplify = FALSE)  # Keep the result as a list of data frames
    
    return(samples_list)
  })
  
  # Return the list of repeated sampled data frames
  return(repeated_samples)
}

# test the call
sample_fxn_test <- sample_fxn(metrics_df_wide, n = 2000, num_reps = 500)

```

Compute stats
```{r}
# Function to compute quantiles for the specified columns
compute_quantiles_for_columns <- function(sampled_data, cols = c('2021-2040', '2041-2060', '2081-2100'), probs = c(0.05, 0.5, 0.95)) {
  
  # Extract the specified columns from the sampled data
  sampled_columns <- sampled_data[, cols, drop = FALSE]
  
  # Compute quantiles for each column
  quantiles_result <- sapply(sampled_columns, function(col) {
    quantile(col, probs = probs, na.rm = TRUE)  # Compute the quantiles, removing NAs if any
  })
  
  return(quantiles_result)
}

# Now, apply this function to the repeated samples
compute_quantiles_for_repeated_samples <- function(repeated_samples, cols = c('2021-2040', '2041-2060', '2081-2100'), probs = c(0.05, 0.5, 0.95)) {
  
  # Loop over the repeated samples (top level: scenarios)
  quantiles_all_scenarios <- lapply(repeated_samples, function(scenario_samples) {
    
    # Loop over each repetition for the current scenario (secondary level: repetitions)
    scenario_quantiles <- lapply(scenario_samples, function(sampled_data) {
      
      # Compute quantiles for the current sample
      compute_quantiles_for_columns(sampled_data, cols, probs)
    })
    
    return(scenario_quantiles)
  })
  
  return(quantiles_all_scenarios)
}

# Example usage:
# Assuming repeated_samples_result contains the repeated samples for each scenario
quantiles_result <- compute_quantiles_for_repeated_samples(sample_fxn_test)

```


## New subsamplign strategy

**NOT WORKING**
```{r}
optimized_sample_fxn <- function(df, n = 1000, num_reps = 100, target_quantiles_df) {
  
  # Split ensemble data by scenario
  split_df <- split(df, df$scenario)
  
  # Apply sampling separately for each scenario
  repeated_samples <- lapply(names(split_df), function(scenario) {
    
    scenario_df <- split_df[[scenario]]
    
    # Extract target quantiles correctly for the scenario
    target_quantiles <- target_quantiles_df %>%
      filter(scenario == !!scenario) %>%
      select(-scenario) %>%  # Fix: Remove extra column
      column_to_rownames("year_range")  # Ensure year_range is rownames for lookup

    # Repeat sampling process
    samples_list <- replicate(num_reps, {
      
      # Sample n ensemble members randomly
      sampled_data <- scenario_df %>% sample_n(n, replace = FALSE)
      
      # Compute quantiles for the sample
      sample_quantiles <- sapply(sampled_data[, c('2021-2040', '2041-2060', '2081-2100')], 
                                 function(col) quantile(col, probs = c(0.05, 0.5, 0.95), na.rm = TRUE))
      
      # Convert quantiles to proper format
      sample_quantiles <- as.data.frame(t(sample_quantiles))  # Transpose so years are rows

      # Debugging: Check if dimensions match
      if (!all(dim(sample_quantiles) == dim(target_quantiles))) {
        print("Mismatch detected:")
        print(dim(sample_quantiles))
        print(dim(target_quantiles))
      }
      
      # Check if sample aligns with scenario-specific IPCC quantiles
      if (all(abs(sample_quantiles - target_quantiles) < 0.1)) {
        return(sampled_data)
      } else {
        return(NULL) # Discard and resample
      }
      
    }, simplify = FALSE)
    
    # Remove NULL results (failed samples)
    samples_list <- samples_list[!sapply(samples_list, is.null)]
    
    return(samples_list)
  })
  
  return(repeated_samples)
}

# Run optimized sampling function
optimized_samples <- optimized_sample_fxn(metrics_df_wide, n = 2000, num_reps = 500, target_quantiles_df = target_IPCC_quantiles_df)
```

## Simplified structure 

```{r}
# Reshape data
metrics_df_wide <- metrics_df %>%
  pivot_wider(
    names_from = term_length, 
    values_from = metric_result
  )

# Filter metrics_df_wide to SSP1-1.9 and only the 2021-2040 period
ssp1_df <- metrics_df_wide %>%
  filter(scenario == "SSP1-1.9") %>%
  select(run_number, scenario, `2021-2040`)

# Filter target IPCC quantiles for SSP1-1.9 and 2021-2040
ssp1_ipcc_quantiles <- target_IPCC_quantiles_df %>%
  filter(scenario == "SSP1-1.9", year_range == "2021-2040") %>%
  select(low, median, high)

# Print to check
print(ssp1_df)
print(ssp1_ipcc_quantiles)
```
```{r}
simplified_sample_fxn <- function(df, n = 1000, num_reps = 100, target_quantiles) {
  
  samples_list <- replicate(num_reps, {
    
    # Randomly sample n members
    sampled_data <- df %>% sample_n(n, replace = FALSE)
    
    # Compute quantiles for the sample
    sample_quantiles <- quantile(sampled_data$`2021-2040`, probs = c(0.05, 0.5, 0.95), na.rm = TRUE)
    
    # Print to debug
    print("Sampled Quantiles:")
    print(sample_quantiles)
    
    # Check if the sample aligns with target quantiles
    if (all(abs(sample_quantiles - target_quantiles) < 0.1)) {
      return(sampled_data)
    } else {
      return(NULL) # Discard and resample
    }
    
  }, simplify = FALSE)
  
  # Remove NULL results (failed samples)
  samples_list <- samples_list[!sapply(samples_list, is.null)]
  
  return(samples_list)
}

# Run the function with SSP1-1.9 and 2021-2040 data
optimized_samples_ssp1 <- simplified_sample_fxn(ssp1_df, n = 1000, num_reps = 100, target_quantiles = ssp1_ipcc_quantiles)
```

```{r}
guided_sample_fxn_no_replacement <- function(df, n = 1000, num_reps = 100, target_quantiles, tolerance = 0.2) {
  
  # Track ensemble members already used
  used_indices <- c()
  
  samples_list <- replicate(num_reps, {
    
    # Exclude previously used samples
    available_df <- df[!(df$run_number %in% used_indices), ]
    
    # If too few members are left, reset available data
    if (nrow(available_df) < n) {
      available_df <- df  # Reset and allow re-selection
      used_indices <<- c()  # Reset used members
    }
    
    # Sample from remaining members without replacement
    sampled_data <- available_df %>%
      group_by(bin) %>%
      sample_n(size = min(n / n_distinct(bin), nrow(available_df)), replace = FALSE) %>%
      ungroup()
    
    # Update used indices
    used_indices <<- c(used_indices, sampled_data$run_number)
    
    # Compute quantiles
    sample_quantiles <- quantile(sampled_data$`2021-2040`, probs = c(0.05, 0.5, 0.95), na.rm = TRUE)
    
    # Print debugging info
    print("Sampled Quantiles:")
    print(sample_quantiles)

    # Adjust tolerance to ensure enough variation
    if (all(abs(sample_quantiles - target_quantiles) < tolerance)) {
      return(sampled_data)
    } else {
      return(NULL)
    }
    
  }, simplify = FALSE)
  
  # Remove NULL results
  samples_list <- samples_list[!sapply(samples_list, is.null)]
  
  return(samples_list)
}

# Run the new function
# # Ensure the bin column is created before running the function
ssp1_df <- ssp1_df %>%
  mutate(bin = ntile(`2021-2040`, 10))  # Divide data into 10 bins

# Check if the bin column is added
print(head(ssp1_df))

optimized_samples_ssp1 <- guided_sample_fxn_no_replacement(ssp1_df, n = 100, num_reps = 50, target_quantiles = ssp1_ipcc_quantiles, tolerance = 0.17)

```

```{r}
# Compute quantiles for all sampled ensembles
quantile_results <- lapply(optimized_samples_ssp1, function(sample) {
  quantiles <- quantile(sample$`2021-2040`, probs = c(0.05, 0.5, 0.95), na.rm = TRUE)
  
  # Convert to a data frame for easy storage
  return(data.frame(
    lower_5 = quantiles[1],
    median_50 = quantiles[2],
    upper_95 = quantiles[3]
  ))
})

# Combine into one data frame
quantile_df <- bind_rows(quantile_results)

# View results
#print(quantile_df)
```

