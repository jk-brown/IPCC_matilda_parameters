---
title: "Idk the name yet"
author: "Joe Brown"
date: "2025-01-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Goal

1. Identify the 2100 temperature anomaly of 15000 member ensemble at quantiles. 

2. Identify the quantiles at which the Hector 15000 member ensemble has temperatures that match AR6 percentiles.

## Load data

Load full result:
```{r}
# load result 
result <- readRDS("output/model_results/initial_result.RDS")

```

source helper functions:
```{r}
# source utils
source("set-up/utils.R")
source("set-up/libraries.R")
```

Subset data to include `global_tas` and normalize data to AR6 reference period:
```{r}
## subset to only include global_tas
gsat_data <- lapply(names(result), function(scenario_name) {
  
  # copy data
  df <- result[[scenario_name]]
  
  # Count NAs before removal
  na_count <- sum(is.na(df))
  print(paste("Number of NAs in", scenario_name, "data:", na_count))
  
  # subset data to include only global temperature
  subset_data = subset(na.omit(df), variable == "global_tas")
  
  subset_data$scenario <- scenario_name
  
  return(subset_data)
})

## copy list element names
names(gsat_data) <- names(result)

## normalize gsat data
gsat_data_normalized <- lapply(gsat_data, function(df) {
  
  normalize_to_reference(df, reference_start = 1995, reference_end = 2014)
  
}) 

rm(result)
```

# Compute warming metrics

Compute warming metrics for AR6 time periods:

```{r}

# initialize metric 
long_warming_metric <- new_metric(var = 'global_tas', years = 2081:2100, op = median)
mid_warming_metric <- new_metric(var = 'global_tas', years = 2041:2060, op = median)
short_warming_metric <- new_metric(var = 'global_tas', years = 2021:2040, op = median)

metric_list <- list('2081-2100' = long_warming_metric, 
                    '2041-2060' = mid_warming_metric, 
                    '2021-2040' = short_warming_metric)

# compute mid-term warming metrics
warming_result <- lapply(names(gsat_data_normalized), function(scenario_name) {
  
  # copy gsat data
  df <- gsat_data_normalized[[scenario_name]]
  
  # compute metrics for each term length for current scenario element 
  metrics_by_term <- lapply(names(metric_list), function(term_length) {

    # copy metric data
    metric <- metric_list[[term_length]]
        
    # compute metrics
    result <- metric_calc(df, metric)
    
    # add term_length column
    result$term_length <- term_length
  
    return(result)  
  })  
  
  # rbind term_lengths for currrent scenario element
  metrics_by_scenario <- do.call(rbind, metrics_by_term)
  
  # Add column for scenario name
  metrics_by_scenario$scenario <- scenario_name
  
  return(metrics_by_scenario)

})

# bind metric result for all scenarios to one df
metrics_df <- do.call(rbind, warming_result)

```

# Compute temp anomaly quantiles

Temperature anomaly quantiles from Hector 15000 member ensemble:

```{r}

warming_quantiles_hector <- metrics_df %>% 
  group_by(scenario, term_length) %>% 
  summarize(
    lower = quantile(metric_result, probs = 0.15), 
    central = quantile(metric_result, probs = 0.5), 
    upper = quantile(metric_result, probs = 0.95),
  .groups = "drop")

```

```{r}
# Display the computed quantiles as a table
library(knitr)

# IPCC data 
source("data/ipcc_data/ipcc_warming_data.R")
IPCC_data <- subset(IPCC_data, name == "WG1 Assessed Range")
names(IPCC_data) <- c()

kable(warming_quantiles_hector, 
      caption = "Temperature Anomaly Quantiles for Hector 15000 Member Ensemble",
      col.names = c("Scenario", "Term Length", "Lower Quantile (5%)", "Central Quantile (50%)", "Upper Quantile (95%)"))

```

```{r}

# Example distribution (your Hector ensemble result)
hector_distribution <- metric_test$`SSP1-1.9.2021-2040`$metric_result

# Value you want to find the quantile for (e.g., 0.38)
value_to_find <- 0.85

# Sort the distribution to prepare for the ECDF
sorted_distribution <- sort(hector_distribution)

# Generate the empirical cumulative distribution function (ECDF)
ecdf_function <- ecdf(sorted_distribution)

# Use the ECDF to find the cumulative probability (percentile) for the given value
percentile_of_value <- ecdf_function(value_to_find)

# Print the percentile (cumulative probability) for the value
cat("The value", value_to_find, "is at approximately", round(percentile_of_value * 100, 2), "percentile of the distribution.\n")

```

```{r}
# Iterate through each item in the ipcc_data_test list
for (scenario_term in names(ipcc_data_test)) {
  # Extract the corresponding Hector distribution (metric_result)
  hector_distribution <- metric_test[[scenario_term]]$metric_result
  
  # Sort the Hector distribution
  sorted_distribution <- sort(hector_distribution)
  
  # Create the ECDF function
  ecdf_function <- ecdf(sorted_distribution)
  
  # Extract the corresponding IPCC values (lower, median, upper)
  ipcc_values <- ipcc_data_test[[scenario_term]]
  lower_ci <- ipcc_values$lower_ci
  median_ci <- ipcc_values$median
  upper_ci <- ipcc_values$upper_ci
  
  # Compute the percentiles for the lower, median, and upper IPCC values
  lower_percentile <- ecdf_function(lower_ci)
  median_percentile <- ecdf_function(median_ci)
  upper_percentile <- ecdf_function(upper_ci)
  
  # Print the percentiles for this scenario and term length
  cat("Scenario:", scenario_term, "\n")
  cat("Lower CI (", lower_ci, ") is at approximately", round(lower_percentile * 100, 2), "percentile\n")
  cat("Median CI (", median_ci, ") is at approximately", round(median_percentile * 100, 2), "percentile\n")
  cat("Upper CI (", upper_ci, ") is at approximately", round(upper_percentile * 100, 2), "percentile\n")
  cat("\n") # To separate results for clarity
}
```


