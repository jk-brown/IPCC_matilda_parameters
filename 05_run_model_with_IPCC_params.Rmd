---
title: "Re-run Model With IPCC Params"
author: "Joe Brown"
date: "2025-03-07"
output: html_document
---

# Goal

In this document we want to re-run the model with the IPCC parameters and determine if all time periods align wiht IPCC values across all of the SSP scenarios.

## Import Parameters

```{r}
params <- read.csv("output/parameters/IPCC_params.csv", stringsAsFactors = F)
  
params <- params %>% 
  select(-X, -run_number)

```

# Set up model

Source files in `set-up` folder.

```{r}
source("set-up/source_all.R")

```

Generate initial parameters to be used for model runs.

Load .ini files:
```{r}
# ini directory 
ini_dir <- paste0(system.file("input", package = "hector"), "/")

# read ini files into a list 
ini_list <- list("SSP1-1.9" = paste0(ini_dir, "hector_ssp119.ini"), 
                 "SSP1-2.6" = paste0(ini_dir, "hector_ssp126.ini"), 
                 "SSP2-4.5" = paste0(ini_dir, "hector_ssp245.ini"), 
                 "SSP3-7.0" = paste0(ini_dir, "hector_ssp370.ini"), 
                 "SSP5-8.5" = paste0(ini_dir, "hector_ssp585.ini"))

```

## Split parameters into separate jobs

Slit parameters into jobs -- will be a list:

```{r}
# splitting params into chunks for parallelization
n_chunks <- min(nrow(params), 100)  # Use the smaller of (a fix to previous error)

split_indices <- rep(1:n_chunks, length.out = nrow(params))  # Evenly assign runs to chunks
param_chunks <- split(params, split_indices)  # Create the chunks

# Running checks to make sure previous error is resolved
print("Number of parameter sets in param_chunks:")
print(length(param_chunks))  # Should match n_chunks

print("Total number of runs across all chunks:")
print(sum(sapply(param_chunks, nrow)))  # Should match nrow(params)

print("Unique run numbers in param_chunks:")
print(length(unique(unlist(lapply(param_chunks, function(x) x$run_number)))))  # Should match nrow(params)
```

```{r}
n_chunks <- min(nrow(params), 100)  # Use a reasonable number of chunks
param_chunks <- split(params, ceiling(seq_along(1:nrow(params)) / (nrow(params) / n_chunks)))

print("Number of parameter sets in param_chunks:")
print(length(param_chunks))  # Should match `n_chunks`

print("Total number of runs across all chunks:")
print(sum(sapply(param_chunks, nrow)))  # Should match `nrow(params)`

print("Check for duplication across chunks:")
all_rows <- do.call(rbind, param_chunks)
print(nrow(unique(all_rows)))  # Should match `nrow(params)`

```
## Set up: Parallel Computing

```{r}

source("set-up/parallel_set_up.R")

```

### Run model using parallel

```{r}
# Run model in parallel mode
result <- parLapply(cl, names(ini_list), function(scenario_name) {
  
  # store data in scenario object
  scenario <- ini_list[[scenario_name]]
  
  # initialize core with the names of the current scenario
  core <- newcore(scenario, name = scenario_name)
  
  # run model for each param chunk for the current core 
  result_list <- lapply(param_chunks, function(chunk) {
    
    iterate_model(core = core, 
                  params = chunk, 
                  save_years = 1800:2200, 
                  save_vars = c("global_tas", 
                                "gmst", 
                                "CO2_concentration", 
                                "ocean_uptake"))
  })
  
  # ensure correct run_number from one chunk to the next
  for (i in 2:length(result_list)) {
    
    max_run_number <- max(result_list[[i - 1]]$run_number)
    
    result_list[[i]]$run_number <- result_list[[i]]$run_number + max_run_number
  }
  
  # bind chunked results into a df for each scenario
  result_return <- do.call(rbind, result_list)
  
  return(result_return)
  
})

# Add names to the list 
names(result) <- names (ini_list)

stopCluster(cl)
```

## Save job

```{r}
result_dir <- here("output", "model_results")

file_name <- "posterior_result_low_ecs.RDS"

saveRDS(result, file.path(result_dir, file_name))

rm(result)
```
